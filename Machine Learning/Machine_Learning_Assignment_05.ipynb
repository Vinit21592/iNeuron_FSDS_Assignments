{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd733ec3",
   "metadata": {},
   "source": [
    "### 1. What are the key tasks that machine learning entails? What does data pre-processing imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb0b213",
   "metadata": {},
   "source": [
    "#### Machine learning involves several key tasks, which can be broadly categorized into the following:\n",
    "\n",
    "**Data Collection:** The first step in any machine learning project is to collect relevant data that can be used to train a model.\n",
    "\n",
    "**Data Preprocessing:** This step involves cleaning the data, handling missing values, and transforming the data into a format that can be used for training.\n",
    "\n",
    "**Feature Engineering:** Feature engineering is the process of selecting and transforming the most relevant features from the data that can help the model learn patterns and make predictions.\n",
    "\n",
    "**Model Selection:** This step involves selecting the appropriate machine learning model that can be used to solve the problem at hand.\n",
    "\n",
    "**Training the Model:** Once the model is selected, it is trained on the prepared dataset.\n",
    "\n",
    "**Model Evaluation:** After training, the model is evaluated on a separate validation dataset to ensure that it is performing well and not overfitting to the training data.\n",
    "\n",
    "**Hyperparameter Tuning:** Hyperparameters are settings that control the behavior of the machine learning model. Tuning them can help to improve the model's performance.\n",
    "\n",
    "**Deployment:** Once the model is trained and evaluated, it can be deployed into a production environment for real-world use.\n",
    "\n",
    "#### Data pre-processing is the step of cleaning and preparing the data before it can be used for training a machine learning model. This step involves several tasks, including:\n",
    "\n",
    "**Data Cleaning:** This involves removing any irrelevant or duplicate data, dealing with missing values, and correcting any errors in the data.\n",
    "\n",
    "**Data Transformation:** This involves converting the data into a format that can be used for machine learning, such as encoding categorical variables, scaling numerical data, and normalizing the data.\n",
    "\n",
    "**Feature Selection:** This involves selecting the most relevant features from the data that can help the model learn patterns and make predictions.\n",
    "\n",
    "**Dimensionality Reduction:** This involves reducing the number of features in the data to simplify the model and improve its performance.\n",
    "\n",
    "By performing these tasks, data pre-processing ensures that the data is of high quality and is suitable for training a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b33ef",
   "metadata": {},
   "source": [
    "### 2. Describe quantitative and qualitative data in depth. Make a distinction between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8c559",
   "metadata": {},
   "source": [
    "The Data Type Is Broadly Classified Into:\n",
    "\n",
    "1. Quantitative\n",
    "2. Qualitative\n",
    "* **Quantitative Data Type:** This Type Of Data Type Consists Of Numerical Values. Anything Which Is Measured By Numbers. E.G., Profit, Quantity Sold, Height, Weight, Temperature, Etc. This Is Again Of Two Types\n",
    "\n",
    "1. **Discrete Data Type:** – The Numeric Data Which Have Discrete Values Or Whole Numbers. This Type Of Variable Value If Expressed In Decimal Format Will Have No Proper Meaning. Their Values Can Be Counted. E.G.: – No. Of Cars You Have, No. Of Marbles In Containers, Students In A Class, Etc.\n",
    "2. **Continuous Data Type:** – The Numerical Measures Which Can Take The Value Within A Certain Range. This Type Of Variable Value If Expressed In Decimal Format Has True Meaning. Their Values Can Not Be Counted But Measured. The Value Can Be Infinite E.G.: Height, Weight, Time, Area, Distance, Measurement Of Rainfall, Etc.\n",
    "* **Qualitative Data Type:** These Are The Data Types That Cannot Be Expressed In Numbers. This Describes Categories Or Groups And Is Hence Known As The Categorical Data Type. This Can Be Divided Into:-\n",
    "\n",
    "1. **Structured Data:** This Type Of Data Is Either Number Or Words. This Can Take Numerical Values But Mathematical Operations Cannot Be Performed On It. This Type Of Data Is Expressed In Tabular Format. E.G.) Sunny=1, Cloudy=2, Windy=3 Or Binary Form Data Like 0 Or1, Good Or Bad, Etc.\n",
    "2. **Unstructured Data:** This Type Of Data Does Not Have The Proper Format And Therefore Known As Unstructured Data.This Comprises Textual Data, Sounds, Images, Videos, Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4ed0c",
   "metadata": {},
   "source": [
    "### 3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12413382",
   "metadata": {},
   "source": [
    "Here is a basic data collection with some sample records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf04bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Income</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Credit Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>50000</td>\n",
       "      <td>Married</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sarah</td>\n",
       "      <td>28</td>\n",
       "      <td>Female</td>\n",
       "      <td>35000</td>\n",
       "      <td>Single</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Michael</td>\n",
       "      <td>42</td>\n",
       "      <td>Male</td>\n",
       "      <td>80000</td>\n",
       "      <td>Married</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Emily</td>\n",
       "      <td>25</td>\n",
       "      <td>Female</td>\n",
       "      <td>25000</td>\n",
       "      <td>Single</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>David</td>\n",
       "      <td>48</td>\n",
       "      <td>Male</td>\n",
       "      <td>90000</td>\n",
       "      <td>Married</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Name  Age  Gender  Income Marital Status  Credit Score\n",
       "0   1     John   35    Male   50000        Married           700\n",
       "1   2    Sarah   28  Female   35000         Single           600\n",
       "2   3  Michael   42    Male   80000        Married           750\n",
       "3   4    Emily   25  Female   25000         Single           550\n",
       "4   5    David   48    Male   90000        Married           800"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create the dataframe\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Name': ['John', 'Sarah', 'Michael', 'Emily', 'David'],\n",
    "    'Age': [35, 28, 42, 25, 48],\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male'],\n",
    "    'Income': [50000, 35000, 80000, 25000, 90000],\n",
    "    'Marital Status': ['Married', 'Single', 'Married', 'Single', 'Married'],\n",
    "    'Credit Score': [700, 600, 750, 550, 800]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeea37b",
   "metadata": {},
   "source": [
    "In this dataset, we have included at least one attribute from each of the machine learning data types:\n",
    "\n",
    "1. **Categorical Data:** The \"Gender\" and \"Marital Status\" attributes are examples of categorical data, as they represent discrete values that cannot be ordered.\n",
    "\n",
    "2. **Numerical Data:** The \"Age\", \"Income\", and \"Credit Score\" attributes are examples of numerical data, as they represent continuous values that can be measured.\n",
    "\n",
    "3. **Ordinal Data:** There is no ordinal data in this dataset, but an example would be a rating scale where the values have a meaningful order, such as \"low\", \"medium\", and \"high\".\n",
    "\n",
    "4. **Time-Series Data:** There is no time-series data in this dataset, but an example would be stock prices over time.\n",
    "\n",
    "5. **Text Data:** There is no text data in this dataset, but an example would be customer reviews or social media posts.\n",
    "\n",
    "By collecting data with a variety of data types, we can ensure that our machine learning model is able to handle diverse types of input and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8800cfa",
   "metadata": {},
   "source": [
    "### 4. What are the various causes of machine learning data issues? What are the ramifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0b1e8",
   "metadata": {},
   "source": [
    "There are several causes of machine learning data issues, including:\n",
    "\n",
    "1. **Missing Data:** This occurs when some data points or attributes are not recorded or available. This can lead to biased or incomplete models, as the missing data may have contained important information.\n",
    "\n",
    "2. **Duplicate Data:** This occurs when there are multiple records that are identical or almost identical. This can lead to overfitting and biased models, as the duplicated data may be over-represented in the training set.\n",
    "\n",
    "3. **Inconsistent Data:** This occurs when the same data is recorded in different formats or with different units of measurement. This can lead to inaccurate models, as the inconsistent data may be difficult to compare or combine.\n",
    "\n",
    "4. **Imbalanced Data:** This occurs when the distribution of the data is uneven, with some classes or categories being over-represented or under-represented. This can lead to biased models, as the imbalanced data may result in poor performance on the under-represented classes.\n",
    "\n",
    "The ramifications of these issues can include:\n",
    "\n",
    "1. **Poor Model Performance:** Machine learning models that are trained on poor-quality data may perform poorly, leading to inaccurate or unreliable predictions.\n",
    "\n",
    "2. **Biased Models:** Machine learning models that are trained on biased data may produce biased predictions, which can have serious consequences in areas such as criminal justice, healthcare, and hiring.\n",
    "\n",
    "3. **Increased Costs:** Poor-quality data can result in wasted time, resources, and money, as machine learning models may need to be retrained or adjusted to compensate for data issues.\n",
    "\n",
    "4. **Damage to Reputation:** Poor-quality data can damage the reputation of organizations that rely on machine learning, as customers, stakeholders, and regulators may lose confidence in the accuracy and reliability of the models.\n",
    "\n",
    "To address these issues, it is important to perform data cleaning, data validation, and data augmentation techniques before using data for machine learning models. These techniques can help ensure that the data is of high quality, consistent, and balanced, which can improve the accuracy and reliability of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb3c00",
   "metadata": {},
   "source": [
    "### 5. Demonstrate various approaches to categorical data exploration with appropriate examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc104ad",
   "metadata": {},
   "source": [
    "Exploring categorical data is an important step in any data analysis or machine learning project. Here are some approaches to categorical data exploration, along with examples:\n",
    "\n",
    "1. **Frequency Tables:** Frequency tables show the number of times each category appears in the data. For example, if we have a dataset of car sales that includes the make and model of each car sold, we can create a frequency table to see how many sales there were for each make and model.\n",
    "\n",
    "2. **Bar Charts:** Bar charts are a visual representation of frequency tables, where each category is represented by a bar and the height of the bar corresponds to the frequency of the category. Continuing with the car sales example, we can create a bar chart to see the distribution of sales across different makes and models.\n",
    "\n",
    "3. **Pie Charts:** Pie charts are another way to visualize frequency tables, where each category is represented as a slice of a pie and the size of the slice corresponds to the frequency of the category. Pie charts are useful when we want to see the proportion of each category in the data. For example, we can create a pie chart to see the proportion of car sales for each make.\n",
    "\n",
    "4. **Cross-tabulation Tables:** Cross-tabulation tables, also known as contingency tables, show the frequency of each combination of categories. Continuing with the car sales example, we can create a cross-tabulation table to see how many sales there were for each make and model combination.\n",
    "\n",
    "5. **Stacked Bar Charts:** Stacked bar charts are a way to visualize cross-tabulation tables, where each category is represented by a stacked bar and the height of each segment of the bar corresponds to the frequency of the category combination. Stacked bar charts are useful when we want to see the distribution of each category combination in the data. For example, we can create a stacked bar chart to see the distribution of car sales across different makes and models.\n",
    "\n",
    "6. **Heat Maps:** Heat maps are a way to visualize cross-tabulation tables using color-coded cells. Each cell corresponds to a category combination, and the color of the cell corresponds to the frequency of the combination. Heat maps are useful when we want to see the distribution of categories across multiple dimensions. For example, we can create a heat map to see the distribution of car sales across different makes, models, and colors.\n",
    "\n",
    "These are just some of the approaches to categorical data exploration. The choice of approach will depend on the nature of the data and the research questions or goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec661b29",
   "metadata": {},
   "source": [
    "### 6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc73304",
   "metadata": {},
   "source": [
    "Missing values in variables can have a significant impact on the learning activity, especially in machine learning models. Here are some ways in which missing values can affect the learning activity:\n",
    "\n",
    "1. **Bias:** If the missing values are not handled properly, they can lead to biased models. For example, if a certain group of observations has a higher proportion of missing values than another group, the model may be biased towards the group with fewer missing values.\n",
    "\n",
    "2. **Inaccuracy:** Missing values can lead to inaccurate models if they are not imputed properly. For example, if missing values are simply removed, this can result in a loss of information and a reduction in the sample size, which can lead to inaccurate predictions.\n",
    "\n",
    "3. **Reduced Power:** If a large proportion of the data is missing, this can reduce the statistical power of the model, making it difficult to detect significant effects or relationships.\n",
    "\n",
    "To address missing values, there are several techniques that can be used, including:\n",
    "\n",
    "1. **Deletion:** This involves simply removing the observations or variables with missing values. This approach is quick and easy, but it can result in a loss of information and a reduction in statistical power.\n",
    "\n",
    "2. **Imputation:** This involves replacing missing values with estimates based on the available data. There are several methods of imputation, including mean imputation, regression imputation, and multiple imputation.\n",
    "\n",
    "3. **Data Augmentation:** This involves generating synthetic data to replace missing values. This approach is particularly useful in cases where the missing values are informative, and can help to avoid bias in the model.\n",
    "\n",
    "4. **Feature Engineering:** This involves creating new features based on the available data, which can help to capture the information that is missing in the original data. For example, if a variable has missing values, a new variable could be created based on the available data that captures a similar concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658c7e0",
   "metadata": {},
   "source": [
    "### 7. Describe the various methods for dealing with missing data values in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf242c",
   "metadata": {},
   "source": [
    "The Various Methods for dealing with missing data values are:\n",
    "\n",
    "1. **Delete the observations:** If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model. For Example Implement this method in a given dataset, we can delete the entire row which contains missing values.\n",
    "2. **Replace missing values with the most frequent value:** You can always impute them based on Mode in the case of categorical variables, just make sure you don’t have highly skewed class distributions.\n",
    "3. **Develop a model to predict missing values:** One smart way of doing this could be training a classifier over your columns with missing values as a dependent variable against other features of your data set and trying to impute based on the newly trained classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc38cbd",
   "metadata": {},
   "source": [
    "### 8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d62d4",
   "metadata": {},
   "source": [
    "Data pre-processing techniques are used to prepare raw data for analysis or machine learning. Here are some commonly used data pre-processing techniques:\n",
    "\n",
    "1. **Data Cleaning:** This involves removing or correcting errors and inconsistencies in the data, such as missing values, incorrect data types, or outliers.\n",
    "\n",
    "2. **Data Transformation:** This involves transforming the data to improve its quality or suitability for analysis, such as scaling or normalizing the data, or converting categorical variables to numerical variables.\n",
    "\n",
    "3. **Feature Selection:** This involves selecting a subset of the available features that are most relevant or useful for the analysis or machine learning task.\n",
    "\n",
    "4. **Dimensionality Reduction:** This involves reducing the number of features or variables in the data while retaining as much information as possible. This can be done using techniques such as principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE).\n",
    "\n",
    "5. **Data Integration:** This involves combining data from multiple sources or formats to create a single, unified dataset for analysis or machine learning.\n",
    "\n",
    "6. **Data Discretization:** This involves converting continuous variables into discrete variables, which can be useful for certain types of analysis or machine learning tasks.\n",
    "\n",
    "Dimensionality reduction is a technique used to reduce the number of features or variables in the data while retaining as much information as possible. This is important because high-dimensional data can be difficult to visualize and analyze, and can lead to overfitting in machine learning models. Dimensionality reduction techniques such as PCA or t-SNE can be used to compress the data into a lower-dimensional space while retaining as much information as possible.\n",
    "\n",
    "Function selection, also known as feature selection, is the process of selecting a subset of the available features that are most relevant or useful for the analysis or machine learning task. This is important because using all available features can lead to overfitting, where the model is too complex and fits the training data too closely, resulting in poor performance on new data. Function selection techniques such as forward selection, backward elimination, or regularization can be used to select the most relevant features and improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa2d7a",
   "metadata": {},
   "source": [
    "### 9. Make brief notes on of the following ?\n",
    "\n",
    "i. What is the IQR? What criteria are used to assess it?\n",
    "\n",
    "ii. Describe the various components of a box plot in detail? When will the lower whisker\n",
    "surpass the upper whisker in length? How can box plots be used to identify outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d89dcd",
   "metadata": {},
   "source": [
    "The following is the brief notes on the following topics:\n",
    "\n",
    "**What is the IQR? What criteria are used to assess it?**\n",
    "\n",
    "* Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "* Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.\n",
    "* The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45cac0",
   "metadata": {},
   "source": [
    "**Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker in length? How can box plots be used to identify outliers?**\n",
    "\n",
    "A box plot is a graphical representation of a dataset that shows the distribution of the data, including the minimum and maximum values, the median, and the quartiles. The box plot consists of several components:\n",
    "\n",
    "1. **Minimum and maximum values:** These are represented by the two lines, called whiskers, that extend from the box. The minimum and maximum values represent the range of the data.\n",
    "\n",
    "2. **Lower quartile (Q1) and upper quartile (Q3):** These are represented by the lower and upper edges of the box, respectively. The interquartile range (IQR) is the distance between Q1 and Q3.\n",
    "\n",
    "3. **Median:** This is represented by the line inside the box. The median is the midpoint of the data, such that half of the data falls above the median and half falls below.\n",
    "\n",
    "4. **Outliers:** These are data points that fall outside the whiskers. They are represented as individual points in the plot.\n",
    "\n",
    "The lower whisker will surpass the upper whisker in length when the data is highly skewed and has a large number of outliers on the lower end of the distribution. This indicates that the majority of the data is concentrated in a smaller range, while a few extreme values are pulling the lower whisker down.\n",
    "\n",
    "Box plots can be used to identify outliers by looking for data points that fall outside the whiskers. Outliers can be identified as individual points in the plot or by using a rule of thumb such as 1.5 times the IQR away from the nearest quartile. Outliers can indicate errors in the data or important features of the distribution that should be further investigated. They can also have a significant impact on statistical analyses and machine learning models, and should be handled appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1b37d",
   "metadata": {},
   "source": [
    "### 10. Make brief notes on any two of the following:\n",
    "\n",
    "1. Data collected at regular intervals\n",
    "\n",
    "2. The gap between the quartiles\n",
    "\n",
    "3. Use a cross-tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eed9ba",
   "metadata": {},
   "source": [
    "The following are the breif notes about:\n",
    "\n",
    "**Data collected at regular intervals:**\n",
    "\n",
    "* Interval data is one of the two types of discrete data.\n",
    "* An example of interval data is the data collected on a thermometer—its gradation or markings are equidistant.\n",
    "* Unlike ordinal data, interval data always take numerical values where the distance between two points on the scale is standardised and equal.\n",
    "\n",
    "**The gap between the quartiles:**\n",
    "\n",
    "* Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "* Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.\n",
    "* The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bacc351",
   "metadata": {},
   "source": [
    "### 11. Make a comparison between:\n",
    "\n",
    "1. Data with nominal and ordinal values\n",
    "\n",
    "2. Histogram and box plot\n",
    "\n",
    "3. The average and median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b57724",
   "metadata": {},
   "source": [
    "The following are the breif notes about:\n",
    "\n",
    "**The average and median:**\n",
    "\n",
    "* The mean (informally, the “average“) is found by adding all of the numbers together and dividing by the number of items in the set: 10 + 10 + 20 + 40 + 70 / 5 = 30. The median is found by ordering the set from lowest to highest and finding the exact middle. The median is just the middle number: 20\n",
    "\n",
    "**Histogram and barplot:**\n",
    "\n",
    "* Histograms and box plots are very similar in that they both help to visualize and describe numeric data. Although histograms are better in determining the underlying distribution of the data, box plots allow you to compare multiple data sets better than histograms as they are less detailed and take up less space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
